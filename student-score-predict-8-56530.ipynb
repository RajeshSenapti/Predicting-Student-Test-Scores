{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":119082,"databundleVersionId":14993753,"sourceType":"competition"},{"sourceId":10909264,"sourceType":"datasetVersion","datasetId":6780703},{"sourceId":13904981,"sourceType":"datasetVersion","datasetId":8762382}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import TargetEncoder\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nnp.random.seed(42)\n\ntrain_df = pd.read_csv(\"/kaggle/input/playground-series-s6e1/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/playground-series-s6e1/test.csv\")\noriginal_df = pd.read_csv(\"/kaggle/input/exam-score-prediction-dataset/Exam_Score_Prediction.csv\")\noriginal_df2 = pd.read_csv(\"/kaggle/input/student-performance-prediction/student_performance_dataset.csv\")\n\nsubmission_df = pd.read_csv(\"/kaggle/input/playground-series-s6e1/sample_submission.csv\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T06:25:44.591532Z","iopub.execute_input":"2026-01-31T06:25:44.591889Z","iopub.status.idle":"2026-01-31T06:25:46.417510Z","shell.execute_reply.started":"2026-01-31T06:25:44.591860Z","shell.execute_reply":"2026-01-31T06:25:46.416445Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Train shape:    {train_df.shape}\")\nprint(f\"Test shape:     {test_df.shape}\")\nprint(f\"Original shape: {original_df.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T06:25:46.419550Z","iopub.execute_input":"2026-01-31T06:25:46.419920Z","iopub.status.idle":"2026-01-31T06:25:46.426228Z","shell.execute_reply.started":"2026-01-31T06:25:46.419884Z","shell.execute_reply":"2026-01-31T06:25:46.425100Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TARGET = \"exam_score\"\nID_COL = \"id\"\n\nbase_features = [col for col in train_df.columns if col not in [TARGET, ID_COL]]\nCATS = train_df.select_dtypes(\"object\").columns.to_list()\n\nprint(f\"\\nBase features: {len(base_features)}\")\nprint(f\"Categorical features: {CATS}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T06:25:46.427524Z","iopub.execute_input":"2026-01-31T06:25:46.427905Z","iopub.status.idle":"2026-01-31T06:25:46.493831Z","shell.execute_reply.started":"2026-01-31T06:25:46.427868Z","shell.execute_reply":"2026-01-31T06:25:46.493017Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_optimized(df):\n    \"\"\"\n    Generate high-value features INCLUDING binned features.\n    Returns: (DataFrame with selected features, list of numeric feature names)\n    \"\"\"\n    df_temp = df.copy()\n    eps = 1e-5\n\n    # Polynomials (2nd order only)\n    df_temp['study_hours_squared'] = df_temp['study_hours'] ** 2\n    df_temp['class_attendance_squared'] = df_temp['class_attendance'] ** 2\n    df_temp['sleep_hours_squared'] = df_temp['sleep_hours'] ** 2\n    df_temp['age_squared'] = df_temp['age'] ** 2\n\n    # Log transforms\n    sh_pos = df_temp['study_hours'].clip(lower=0)\n    ca_pos = df_temp['class_attendance'].clip(lower=0)\n    sl_pos = df_temp['sleep_hours'].clip(lower=0)\n\n    df_temp['log_study_hours'] = np.log1p(sh_pos)\n    df_temp['log_class_attendance'] = np.log1p(ca_pos)\n    df_temp['log_sleep_hours'] = np.log1p(sl_pos)\n\n    # Sqrt transforms\n    df_temp['sqrt_study_hours'] = np.sqrt(sh_pos)\n    df_temp['sqrt_class_attendance'] = np.sqrt(ca_pos)\n\n    # Key interactions\n    df_temp['study_hours_times_attendance'] = df_temp['study_hours'] * df_temp['class_attendance']\n    df_temp['study_hours_times_sleep'] = df_temp['study_hours'] * df_temp['sleep_hours']\n    df_temp['attendance_times_sleep'] = df_temp['class_attendance'] * df_temp['sleep_hours']\n    df_temp['age_times_study_hours'] = df_temp['age'] * df_temp['study_hours']\n\n    # Important ratios\n    df_temp['study_hours_over_sleep'] = df_temp['study_hours'] / (df_temp['sleep_hours'] + eps)\n    df_temp['attendance_over_sleep'] = df_temp['class_attendance'] / (df_temp['sleep_hours'] + eps)\n    df_temp['attendance_over_study'] = df_temp['class_attendance'] / (df_temp['study_hours'] + eps)\n\n    # Ordinal encoding\n    sleep_quality_map = {'poor': 0, 'average': 1, 'good': 2}\n    facility_rating_map = {'low': 0, 'medium': 1, 'high': 2}\n    exam_difficulty_map = {'easy': 0, 'moderate': 1, 'hard': 2}\n\n    df_temp['sleep_quality_numeric'] = df_temp['sleep_quality'].map(sleep_quality_map).fillna(1).astype(int)\n    df_temp['facility_rating_numeric'] = df_temp['facility_rating'].map(facility_rating_map).fillna(1).astype(int)\n    df_temp['exam_difficulty_numeric'] = df_temp['exam_difficulty'].map(exam_difficulty_map).fillna(1).astype(int)\n\n    # Ordinal × numeric interactions\n    df_temp['study_hours_times_sleep_quality'] = df_temp['study_hours'] * df_temp['sleep_quality_numeric']\n    df_temp['attendance_times_facility'] = df_temp['class_attendance'] * df_temp['facility_rating_numeric']\n    df_temp['sleep_hours_times_difficulty'] = df_temp['sleep_hours'] * df_temp['exam_difficulty_numeric']\n\n    # Ordinal × ordinal interactions\n    df_temp['facility_x_sleepq'] = df_temp['facility_rating_numeric'] * df_temp['sleep_quality_numeric']\n    df_temp['difficulty_x_facility'] = df_temp['exam_difficulty_numeric'] * df_temp['facility_rating_numeric']\n\n    # Rule-based flags\n    df_temp[\"high_att_high_study\"] = ((df_temp[\"class_attendance\"] >= 90) & (df_temp[\"study_hours\"] >= 6)).astype(int)\n    df_temp[\"ideal_sleep_flag\"] = ((df_temp[\"sleep_hours\"] >= 7) & (df_temp[\"sleep_hours\"] <= 9)).astype(int)\n    df_temp[\"high_study_flag\"] = (df_temp[\"study_hours\"] >= 7).astype(int)\n\n    # Composite efficiency\n    df_temp['efficiency'] = (df_temp['study_hours'] * df_temp['class_attendance']) / (df_temp['sleep_hours'] + 1)\n\n    # Gap features\n    df_temp['sleep_gap_8'] = (df_temp['sleep_hours'] - 8.0).abs()\n    df_temp['attendance_gap_100'] = (df_temp['class_attendance'] - 100.0).abs()\n\n    # BINNED FEATURES (KEEP THESE - THEY ARE VALUABLE!)\n    df_temp['study_bin_num'] = pd.cut(df_temp['study_hours'], bins=5, labels=False).astype(int)\n    df_temp['attendance_bin_num'] = pd.cut(df_temp['class_attendance'], bins=5, labels=False).astype(int)\n    df_temp['sleep_bin_num'] = pd.cut(df_temp['sleep_hours'], bins=5, labels=False).astype(int)\n    df_temp['age_bin_num'] = pd.cut(df_temp['age'], bins=5, labels=False).astype(int)\n\n    # Feature list (34 features total)\n    numeric_features = [\n        'study_hours_squared', 'class_attendance_squared', 'sleep_hours_squared', 'age_squared',\n        'log_study_hours', 'log_class_attendance', 'log_sleep_hours',\n        'sqrt_study_hours', 'sqrt_class_attendance',\n        'study_hours_times_attendance', 'study_hours_times_sleep', 'attendance_times_sleep',\n        'age_times_study_hours',\n        'study_hours_over_sleep', 'attendance_over_sleep', 'attendance_over_study',\n        'sleep_quality_numeric', 'facility_rating_numeric', 'exam_difficulty_numeric',\n        'study_hours_times_sleep_quality', 'attendance_times_facility', 'sleep_hours_times_difficulty',\n        'facility_x_sleepq', 'difficulty_x_facility',\n        'high_att_high_study', 'ideal_sleep_flag', 'high_study_flag',\n        'efficiency',\n        'sleep_gap_8', 'attendance_gap_100',\n        'study_bin_num', 'attendance_bin_num', 'sleep_bin_num', 'age_bin_num'\n    ]\n\n    return df_temp[base_features + numeric_features], numeric_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T06:25:46.495383Z","iopub.execute_input":"2026-01-31T06:25:46.495831Z","iopub.status.idle":"2026-01-31T06:25:46.516037Z","shell.execute_reply.started":"2026-01-31T06:25:46.495794Z","shell.execute_reply":"2026-01-31T06:25:46.514823Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_raw, numeric_cols = preprocess_optimized(train_df)\ny = train_df[TARGET].reset_index(drop=True)\n\nX_test_raw, _ = preprocess_optimized(test_df)\nX_orig_raw, _ = preprocess_optimized(original_df)\ny_orig = original_df[TARGET].reset_index(drop=True)\n\nfull_data = pd.concat([X_raw, X_test_raw, X_orig_raw], axis=0, ignore_index=True)\n\nfor col in numeric_cols:\n    full_data[col] = full_data[col].astype(float)\n\nX = full_data.iloc[:len(train_df)].copy()\nX_test = full_data.iloc[len(train_df):len(train_df) + len(test_df)].copy()\nX_original = full_data.iloc[len(train_df) + len(test_df):].copy()\n\nprint(f\"Engineered features: {len(numeric_cols)}\")\nprint(f\"Total features: {X.shape[1]} (11 base + {len(numeric_cols)} engineered)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T06:25:46.518555Z","iopub.execute_input":"2026-01-31T06:25:46.518932Z","iopub.status.idle":"2026-01-31T06:25:49.998269Z","shell.execute_reply.started":"2026-01-31T06:25:46.518904Z","shell.execute_reply":"2026-01-31T06:25:49.997415Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FOLDS = 10\nkf = KFold(n_splits=FOLDS, shuffle=True, random_state=1003)\n\noof_pred_lr = np.zeros(X.shape[0])\ntest_preds_lr = np.zeros((X_test.shape[0], FOLDS))\norig_preds_lr = np.zeros(X_original.shape[0])\n\nfor fold, (train_index, val_index) in enumerate(kf.split(X, y), start=1):\n    X_train_fold, X_val = X.iloc[train_index], X.iloc[val_index]\n    y_train_fold, y_val = y.iloc[train_index], y.iloc[val_index]\n\n    X_train_combined = pd.concat([X_train_fold, X_original], axis=0)\n    y_train_combined = pd.concat([y_train_fold, y_orig], axis=0)\n\n    target_encoder = TargetEncoder(smooth='auto', target_type='continuous')\n    X_train_encoded = X_train_combined.copy()\n    X_val_encoded = X_val.copy()\n    X_test_encoded = X_test.copy()\n\n    X_train_encoded[CATS] = target_encoder.fit_transform(X_train_combined[CATS], y_train_combined)\n    X_val_encoded[CATS] = target_encoder.transform(X_val[CATS])\n    X_test_encoded[CATS] = target_encoder.transform(X_test[CATS])\n\n    alphas = np.logspace(-3, 3, 20)\n    lr_model = RidgeCV(alphas=alphas, cv=5, scoring='neg_root_mean_squared_error')\n    lr_model.fit(X_train_encoded, y_train_combined.to_numpy().ravel())\n\n    lr_val_pred = np.clip(lr_model.predict(X_val_encoded), 0, 100)\n    lr_test_pred = np.clip(lr_model.predict(X_test_encoded), 0, 100)\n    lr_orig_pred = np.clip(lr_model.predict(X_train_encoded.iloc[-X_original.shape[0]:]), 0, 100)\n\n    oof_pred_lr[val_index] = lr_val_pred\n    test_preds_lr[:, fold - 1] = lr_test_pred\n    orig_preds_lr += lr_orig_pred / FOLDS\n\n    rmse_lr = np.sqrt(mean_squared_error(y_val, lr_val_pred))\n    print(f\"Fold {fold:2d} | RMSE: {rmse_lr:.6f}\")\n\nlr_oof_rmse = np.sqrt(mean_squared_error(y, oof_pred_lr))\nprint(f\"\\nRidge OOF RMSE: {lr_oof_rmse:.6f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T06:25:49.999393Z","iopub.execute_input":"2026-01-31T06:25:49.999680Z","iopub.status.idle":"2026-01-31T06:32:37.658902Z","shell.execute_reply.started":"2026-01-31T06:25:49.999656Z","shell.execute_reply":"2026-01-31T06:32:37.658042Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# For XGBoost (categorical features as category type)\nfor col in base_features:\n    full_data[col] = full_data[col].astype(str).astype(\"category\")\n\nfor col in numeric_cols:\n    full_data[col] = full_data[col].astype(float)\n\nX_xgb = full_data.iloc[:len(train_df)].copy()\nX_test_xgb = full_data.iloc[len(train_df):len(train_df) + len(test_df)].copy()\nX_original_xgb = full_data.iloc[len(train_df) + len(test_df):].copy()\n\nX_xgb[\"feature_lr_pred\"] = oof_pred_lr\nX_test_xgb[\"feature_lr_pred\"] = test_preds_lr.mean(axis=1)\nX_original_xgb[\"feature_lr_pred\"] = orig_preds_lr\n\nprint(f\"Final feature count: {X_xgb.shape[1]} (including Ridge meta-feature)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T06:32:37.659928Z","iopub.execute_input":"2026-01-31T06:32:37.662302Z","iopub.status.idle":"2026-01-31T06:32:41.278194Z","shell.execute_reply.started":"2026-01-31T06:32:37.662262Z","shell.execute_reply":"2026-01-31T06:32:41.277024Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xgb_params = {\n    \"n_estimators\": 40000,                # more trees – let early stopping decide\n    \"learning_rate\": 0.003,               # slower → better convergence\n    \"max_depth\": 8,                       # slightly shallower to reduce variance\n    \"subsample\": 0.82,                    # ↑ a bit – use more data per tree\n    \"colsample_bytree\": 0.52,             # ↓ slightly – more randomness\n    \"colsample_bynode\": 0.68,             # ↑ slightly – more features per split\n    \"min_child_weight\": 7,                # ↑ a bit – more conservative\n    \"reg_lambda\": 8.0,                    # stronger L2\n    \"reg_alpha\": 0.3,                     # stronger L1\n    \"tree_method\": \"hist\",\n    \"enable_categorical\": True,\n    \"eval_metric\": \"rmse\",\n    \n    \"random_state\": 42,\n    \"early_stopping_rounds\": 250,         # much more patience\n    \"device\": \"cuda\"                      # keep if GPU available\n}\n\ntest_predictions_xgb = []\noof_predictions_xgb = np.zeros(len(X_xgb), dtype=float)\n\nfor fold, (train_index, val_index) in enumerate(kf.split(X_xgb, y), start=1):\n    print(f\"\\nFold {fold:2d}/{FOLDS}\")\n\n    X_train_fold, X_val = X_xgb.iloc[train_index], X_xgb.iloc[val_index]\n    y_train_fold, y_val = y.iloc[train_index], y.iloc[val_index]\n\n    X_train_combined = pd.concat([X_train_fold, X_original_xgb], axis=0)\n    y_train_combined = pd.concat([y_train_fold, y_orig], axis=0)\n\n    model = xgb.XGBRegressor(\n        **xgb_params,\n        callbacks=[xgb.callback.EarlyStopping(rounds=250, save_best=True, maximize=False)])\n    # model.fit(X_train_combined, y_train_combined, eval_set=[(X_val, y_val)], verbose=1000)\n    \n    model.fit(\n        X_train_combined, y_train_combined,\n        eval_set=[(X_val, y_val)],\n        verbose=1000,\n    )\n\n    print(f\"Best iteration: {model.best_iteration}\")\n\n\n    val_preds = model.predict(X_val)\n    oof_predictions_xgb[val_index] = val_preds\n\n    rmse_fold = np.sqrt(mean_squared_error(y_val, val_preds))\n    print(f\"Validation RMSE: {rmse_fold:.5f}\")\n\n    test_predictions_xgb.append(model.predict(X_test_xgb))\n\n    xgb_oof_rmse = np.sqrt(mean_squared_error(y, oof_predictions_xgb))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T06:32:41.279241Z","iopub.execute_input":"2026-01-31T06:32:41.279498Z","iopub.status.idle":"2026-01-31T08:36:11.586731Z","shell.execute_reply.started":"2026-01-31T06:32:41.279471Z","shell.execute_reply":"2026-01-31T08:36:11.585408Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"\\nModel Performance:\")\nprint(f\"  Ridge OOF RMSE:    {lr_oof_rmse:.6f}\")\nprint(f\"  XGBoost OOF RMSE:  {xgb_oof_rmse:.5f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T08:36:11.589519Z","iopub.execute_input":"2026-01-31T08:36:11.589855Z","iopub.status.idle":"2026-01-31T08:36:11.597399Z","shell.execute_reply.started":"2026-01-31T08:36:11.589828Z","shell.execute_reply":"2026-01-31T08:36:11.596309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"\\nFeature Summary:\")\nprint(f\"  Base features:       {len(base_features)}\")\nprint(f\"  Engineered features: {len(numeric_cols)}\")\nprint(f\"  Meta-feature (Ridge): 1\")\nprint(f\"  Total features:      {X_xgb.shape[1]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T08:36:11.598557Z","iopub.execute_input":"2026-01-31T08:36:11.598846Z","iopub.status.idle":"2026-01-31T08:36:11.622118Z","shell.execute_reply.started":"2026-01-31T08:36:11.598821Z","shell.execute_reply":"2026-01-31T08:36:11.621153Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save OOF predictions\noof_xgb = pd.DataFrame({\"id\": train_df[ID_COL], TARGET: oof_predictions_xgb})\noof_xgb.to_csv(\"xgb_oof_optimized.csv\", index=False)\n\n# Save submission\ntest_xgb_avg = np.mean(test_predictions_xgb, axis=0)\nsubmission_xgb = submission_df.copy()\nsubmission_xgb[TARGET] = test_xgb_avg\n# submission_xgb.to_csv(\"submission_optimized.csv\", index=False)\nsubmission_xgb.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T08:39:22.152250Z","iopub.execute_input":"2026-01-31T08:39:22.152633Z","iopub.status.idle":"2026-01-31T08:39:23.904757Z","shell.execute_reply.started":"2026-01-31T08:39:22.152603Z","shell.execute_reply":"2026-01-31T08:39:23.903616Z"}},"outputs":[],"execution_count":null}]}